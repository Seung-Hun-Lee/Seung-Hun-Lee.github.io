<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Temporal Grounding as a Learning Signal for Referring Video Object Segmentation.">
  <meta name="keywords" content="TGL, RVOS, MeViS, Moment Retrieval">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Temporal Grounding as a Learning Signal for Referring Video Object Segmentation</title>
  <style>
    .rotated-text {
      writing-mode: vertical-rl;
      transform: rotate(180deg);
      display: flex;
      align-items: center;
      font-size: 1.5em;
      font-weight: bold;
      margin-right: 10px;
    }

    .video-container {
      display: flex;
      justify-content: center;
      align-items: center;
      flex-wrap: wrap;
      margin-top: 10px;
    }

    .video-wrapper {
      width: 280px;
      /* 고정된 너비 */
      height: 200px;
      /* 고정된 높이 */
      margin: 10px;
      /* 각 비디오 사이 여백 */
      overflow: hidden;
    }

    .video-wrapper video {
      width: 100%;
      /* 비디오 너비 100% */
      height: 100%;
      /* 비디오 높이 100% */
      object-fit: cover;
      /* 비디오 비율 유지 및 오버플로우 처리 */
    }

    .custom-title {
      font-size: 2.5rem;
      /* 원하는 폰트 크기로 조정 */
    }
  </style>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../static/css/index.css">
  <link rel="icon" href="./images/cvlabLOGO.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script src="../static/js/bulma-carousel.min.js"></script>
  <script src="../static/js/bulma-slider.min.js"></script>
  <script src="../static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://seung-hun-lee.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://seung-hun-lee.github.io/projects/CAVIS/">
              CAVIS
            </a>
            <a class="navbar-item" href="https://seung-hun-lee.github.io/projects/LOMM/">
              LOMM
            </a>
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"> Temporal Grounding as a Learning Signal for Referring Video Object Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://seung-hun-lee.github.io">Seunghun Lee</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://Seo-jiwan.github.io">Jiwan Seo</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://Seo-jiwan.github.io">Jeonghoon Kim</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://byeol3325.github.io/">Sungho Moon</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://cvlab.dgist.ac.kr/members/siwon-kim/">Siwon Kim</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://cvlab.dgist.ac.kr/members/haeun-yun/">Haeun Yun</a><sup>1</sup>,
              </span>


              </span>
              </span><br> <!-- 여기서 줄바꿈 추가 -->
              
              <span class="author-block">
                <a href="https://cvlab.dgist.ac.kr/members/hyogyeong-jeon/">Hyogyeong Jeon</a><sup>1</sup>,

              <span class="author-block">
                <a href="https://wonhyeok-choi.github.io/">Wonhyeok Choi</a><sup>1</sup>,
              </span>

                
              
              <span class="author-block">
                <a href="https://jae-hoon-jeong.github.io/">Jaehoon Jeong</a><sup>1</sup>,
              </span>

              <span class="author-block">
                <a href="https://zanedurante.github.io/">Zane Durante</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/mispl/members/professor">Sanghyun Park</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://sunghoonim.github.io">Sunghoon Im</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>DGIST,</span>
              <span class="author-block"><sup>2</sup>Stanford University</span>
            </div>
            <div class="publication-logos" style="margin-top: 10px; display: flex; justify-content: center; gap: 30px;">

            </div>



            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2508.11955"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2508.11955"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> 
              <!-- Code Link. -->

                <span class="link-block">
                  <a href="https://github.com/Seung-Hun-Lee/TGL"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>




  <!--section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section-->

  <section class="section">
    <div class="container is-fluid">
      <!-- <div class="columns is-centered has-text-centered">
            <div class="column is-full">
                <h2 class="title is-3 has-text-centered">Demos</h2>
            </div>
        </div> -->

      <!-- <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <h2 class="title is-3 has-text-centered">Key Mechanism</h2>
            <video autoplay controls muted loop playsinline style="width: 100%; height: auto;">
              <source src="./videos/apt.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div> -->
      <!-- <div style="height: 30px;"></div> -->




      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-nine-tenths">
              <h2 class="title is-3"> Demo </h2>
              <video id="v_6" autoplay controls muted loop playsinline
                style="width: 100%; height: 550px; object-fit: cover;">
                <source src="./videos/Comparison_Video.mp4" type="video/mp4">
              </video>

            </div>
          </div>
        </div>
      </section>




      <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="./static/images/motivation.png"
           class="interpolation-image"
           alt="Interpolate start reference image."/>
      </div>
    </div>
  </section> -->




      <section class="section">
        <div class="container is-max-desktop">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                <p style="margin-top: 10px; font-size: clamp(1rem, 2.5vw, 1.3rem);">
                  Referring Video Object Segmentation (RVOS) aims to segment and track objects in videos based on natural language expressions, requiring precise alignment between visual content and textual queries. However, existing methods often suffer from semantic misalignment, largely due to indiscriminate frame sampling and supervision of all visible objects during training—regardless of their actual relevance to the expression. We identify the core problem as the absence of an explicit temporal learning signal in conventional training paradigms. To address this, we introduce MeViS-M, a dataset built upon the challenging MeViS benchmark, where we manually annotate temporal spans when each object is referred to by the expression. These annotations provide a direct, semantically grounded supervision signal that was previously missing. To leverage this signal, we propose Temporally Grounded Learning (TGL), a novel learning framework that directly incorporates temporal grounding into the training process. Within this frame- work, we introduce two key strategies. First, Moment-guided Dual-path Propagation (MDP) improves both grounding and tracking by decoupling language-guided segmentation for relevant moments from language-agnostic propagation for others. Second, Object-level Selective Supervision (OSS) supervises only the objects temporally aligned with the expression in each training clip, thereby reducing semantic noise and reinforcing language-conditioned learning. Extensive experiments demonstrate that our TGL framework effectively leverages temporal signal to establish a new state-of-the-art on the challenging MeViS benchmark. We will make our code and the MeViS-M dataset publicly available.
                </p>
              </div>
            </div>
          </div>
          <!--/ Abstract. -->

          <!-- Paper video. -->
          <!-- div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
        </div>
      </section>







      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-nine-tenths">
              <h2 class="title is-3">Motivation</h2>
              <img src="./images/motivation.png" class="interpolation-image" alt="Interpolate start reference image." />
              <div class="content has-text-justified">
                <p style="margin-top: 10px; font-size: clamp(1rem, 2.5vw, 1.3rem);">
                  <strong>Importance of moment-aware approach.</strong> (a) Most existing methods rely on random frame sampling, leading to unnatural learning dynamics by forcing models to segment referred objects even in frames unrelated to the given text. (b) Our method explicitly focuses on text-relevant moments to enable semantically and temporally grounded segmentation.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>



      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-nine-tenths">
              <h2 class="title is-3">Dataset & Analysis</h2>
              <img src="./images/Dataset.png" class="interpolation-image" alt="Interpolate start reference image." />
              <div class="content has-text-justified">
                <p style="margin-top: 10px; font-size: clamp(1rem, 2.5vw, 1.3rem);">
                  <strong>MeViS-M dataset and analysis on valid set.</strong> (a) A moment annotation example from MeViS-M, showing temporal spans labeled for each object referred to by the given expression. (b) Comparison of top-1 accuracy for key frame selection on VLMs. The consistently low accuracy across all models underscores the limitation of existing VLMs in moment retrieval and highlights the necessity of fine-grained moment annotations.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>


      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-nine-tenths">
              <h2 class="title is-3">Method</h2>
              <img src="./images/Method.png" class="interpolation-image" alt="Interpolate start reference image." />
              <div class="content has-text-justified">
                <p style="margin-top: 10px; font-size: clamp(1rem, 2.5vw, 1.3rem);">
                  <strong>Overall pipeline.</strong> In (a), \( \mathbf{F}_{\text{Adp}} \) of text-relevant frames are
                  utilized for mask generation and memory update, while text-irrelevant frames employ \(
                  \mathbf{F}_{\text{SAM}} \) for mask generation without contributing to the memory update. (b)
                  illustrates
                  how \( \mathbf{F}_{\text{Adp}} \) and \( \mathbf{F}_{\text{SAM}} \) are extracted from relevant and
                  irrelevant frames, respectively, and how visual features are integrated into the prompt.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
      <!-- Content below is for MathJax -->
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
      </script>
      <!-- End -->





      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-nine-tenths">
              <h2 class="title is-3">Performance</h2>
              <img src="./images/main_table.png" class="interpolation-image" alt="Interpolate start reference image." />
              <div class="content has-text-justified">
                <p style="margin-top: 10px; font-size: clamp(1rem, 2.5vw, 1.3rem);">
                  <strong>Comparison on the MeViS dataset.</strong> Oracle uses ground-truth moments from MeViS-M at
                  inference.
                  <strong>†</strong> indicates methods that leverage Vision-Language Models (VLMs) for keyframe
                  selection.
                  Oracle + Ours† uses VLMs to extract keyframes from ground-truth moments.
                  We adopt Chrono (Meinardus et al. 2024) and BLIP-2 (Li et al. 2023) as keyframe selectors.
                </p>
              </div>
            </div>
          </div>
        </div>
        </section>


        <!-- Analysis -->
        <section class="section">
          <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
              <div class="column is-nine-tenths">
                <h2 class="title is-3">Further Analysis</h2>
                <img src="./images/visualization.png" class="interpolation-image"
                  alt="Interpolate start reference image." />
                <div class="content has-text-justified">
                  <p style="margin-top: 10px; font-size: clamp(0.7rem, 2.0vw, 1.0rem); text-align: center;">
                    <strong>PCA-based feature maps and segmentation results of SAMWISE & TGL.</strong>
                  </p>
                </div>
              </div>
            </div>
          </div>
          </section>
          <!--  -->









          <!-- section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section -->


          <footer class="footer">
            <div class="container">
              <!--div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
  </div-->
              <div class="columns is-centered">
                <div class="column is-8">
                  <div class="content">
                    <p>
                      Video sources include clips from MeViS.
                    </p>


                    <p>
                      This website is licensed under a <a rel="license"
                        href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                      We use the public template of <a href="https://github.com/nerfies/nerfies.github.io">Nertifies
                        Project</a>.
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </footer>

</body>

</html>
